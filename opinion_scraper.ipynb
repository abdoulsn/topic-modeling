{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hindu-aruba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from -r requirements.txt (line 2)) (4.58.0)\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (1.20.2)\n",
      "Requirement already satisfied: pandas in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (1.2.2)\n",
      "Requirement already satisfied: urllib3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from -r requirements.txt (line 9)) (1.26.3)\n",
      "Requirement already satisfied: lxml in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from -r requirements.txt (line 10)) (4.6.2)\n",
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from -r requirements.txt (line 11)) (2.25.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from -r requirements.txt (line 13)) (4.9.3)\n",
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from -r requirements.txt (line 15)) (3.5)\n",
      "Collecting pylint\n",
      "  Downloading pylint-2.7.4-py3-none-any.whl (346 kB)\n",
      "\u001b[K     |████████████████████████████████| 346 kB 508 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from beautifulsoup4->-r requirements.txt (line 13)) (2.2)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 15)) (7.1.2)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 15)) (1.0.1)\n",
      "Requirement already satisfied: regex in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from nltk->-r requirements.txt (line 15)) (2020.11.13)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pandas->-r requirements.txt (line 6)) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas->-r requirements.txt (line 6)) (1.15.0)\n",
      "Collecting mccabe<0.7,>=0.6\n",
      "  Using cached mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Collecting astroid<2.7,>=2.5.2\n",
      "  Downloading astroid-2.5.3-py3-none-any.whl (226 kB)\n",
      "\u001b[K     |████████████████████████████████| 226 kB 774 kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: toml>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from pylint->-r requirements.txt (line 16)) (0.10.2)\n",
      "Collecting isort<6,>=4.2.5\n",
      "  Downloading isort-5.8.0-py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 843 kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting lazy-object-proxy>=1.4.0\n",
      "  Downloading lazy-object-proxy-1.6.0.tar.gz (44 kB)\n",
      "\u001b[K     |████████████████████████████████| 44 kB 1.8 MB/s eta 0:00:01\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: wrapt<1.13,>=1.11 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from astroid<2.7,>=2.5.2->pylint->-r requirements.txt (line 16)) (1.12.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->-r requirements.txt (line 11)) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->-r requirements.txt (line 11)) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages (from requests->-r requirements.txt (line 11)) (2020.12.5)\n",
      "Building wheels for collected packages: lazy-object-proxy\n",
      "\u001b[33m  WARNING: Building wheel for lazy-object-proxy failed: [Errno 13] Permission denied: '/Users/abdoulayediallo/Library/Caches/pip/wheels/25'\u001b[0m\n",
      "Failed to build lazy-object-proxy\n",
      "\u001b[31mERROR: Could not build wheels for lazy-object-proxy which use PEP 517 and cannot be installed directly\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funky-jerusalem",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For jupyter loader et warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Python numpy et pandas respctivement pout Lin Algebra et dataframe\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 10000)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "# url spiders \n",
    "from urllib.parse import urlencode, urlparse, parse_qs\n",
    "from lxml.html import fromstring\n",
    "from requests import get\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen \n",
    "from lxml.html import fromstring\n",
    "import requests\n",
    "from itertools import cycle\n",
    "import traceback\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "muslim-speaking",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.opinion-assurances.fr/assureur-allianz.html'\n",
    "\n",
    "def parse_url(url):\n",
    "    page = requests.get(url).text\n",
    "    return bs(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "polyphonic-farmer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "coms = p.find_all('div', class_={'oa_reactionText', 'oa_stackLevel'})\n",
    "for i in coms:\n",
    "    # print(i)\n",
    "    rating = i.find('span')\n",
    "    print(len(rating.i.get('class')[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "focused-bibliography",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-072d201156c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"row oa_reactionBlock\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"oa_reactionText\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#type(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/bs4/element.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2172\u001b[0m         \u001b[0;34m\"\"\"Raise a helpful exception to explain a common code fix.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2173\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   2174\u001b[0m             \u001b[0;34m\"ResultSet object has no attribute '%s'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2175\u001b[0m         )\n",
      "\u001b[0;31mAttributeError\u001b[0m: ResultSet object has no attribute 'find'. You're probably treating a list of elements like a single element. Did you call find_all() when you meant to call find()?"
     ]
    }
   ],
   "source": [
    "a = p.find_all(class_ = \"row oa_reactionBlock\").find(\"oa_reactionText\")\n",
    "#type(a)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "modular-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "note = []\n",
    "date = []\n",
    "textes = []\n",
    "categories = []\n",
    "\n",
    "def get_data(url):\n",
    "    '''\n",
    "    input: links of liens_categories\n",
    "    Output: dictionary of title and link to content article\n",
    "    '''\n",
    "        \n",
    "    page = parse_url(url)\n",
    "    print(page.find_all('oa_reactionText'))\n",
    "\n",
    "\n",
    "    #return titles, link_to_content, textes, categories # A arranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "approximate-headset",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-agent",
   "metadata": {},
   "outputs": [],
   "source": [
    "            categorie = url.split('/')[-2]\n",
    "            t = soup.text.strip()\n",
    "            l = soup.find('a')['href']\n",
    "            titles.append(t)\n",
    "            link_to_content.append(l)\n",
    "            # get text data\n",
    "            content = parse_url(l)\n",
    "            categories.append(categorie)\n",
    "            texte = [a.text for a in content.find(class_ = \"entry-content\").find_all('p') if a]\n",
    "            textes.append(texte)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
